# 新規記事提案機能の実装アイデア

## 概要

Search Consoleのデータと既存記事の分析から、SEO対策として効果的な新規記事のタイトルや内容を提案する機能。

## 要件

- コストや手間をかけずに実装
- 精度が出るかどうか
- コスパが合うかどうか

## 重要な前提条件

**GSC APIでのキーワード取得について**:
- 現在の `getKeywordData()` は特定ページのキーワードのみを取得（`pageUrl` が必須）
- **ドメイン全体のキーワードを取得するには、`pageUrl` を指定しない必要がある**
- 実装時は、`pageUrl` をオプショナルにした新規メソッド `getAllKeywords()` を追加する必要がある
- これにより、**ドメインに属するすべての記事のキーワードデータを一度に取得可能**

**実装方法**:
- `getKeywordData()` の `pageUrl` パラメータをオプショナルにする
- `pageUrl` が未指定の場合、`query()` メソッド内でページフィルタを追加しない（既存の `getPageUrls()` と同じ仕組み）
- これにより、ドメイン全体のキーワードを取得できる

**URL入力の処理**:
- ユーザーが特定の記事URLを入力した場合でも、ドメインだけを抜き出して処理可能
- 既存の `extractDomain()` 関数（`lib/competitor-filter.ts`）を活用
- 入力例:
  - `https://example.com/article/123` → `https://example.com/` または `sc-domain:example.com`
  - `example.com/article/123` → `example.com` → `sc-domain:example.com` または `https://example.com/`
- GSC APIの形式に変換:
  - `sc-domain:` 形式: `sc-domain:example.com`
  - URLプロパティ形式: `https://example.com/`

**コストと処理時間について**:
- **コスト**: 無料（GSC APIは無料）
- **処理時間**: 
  - 小規模サイト（キーワード数 < 1000）: 1-3秒程度
  - 中規模サイト（キーワード数 1000-10000）: 3-10秒程度（ページネーションが必要）
  - 大規模サイト（キーワード数 > 10000）: 10-30秒程度（複数回のAPI呼び出しが必要）
- **API制限**: 
  - `rowLimit` の最大値: 25,000（ただし、1回のリクエストで取得できるのは最大25,000行）
  - ページネーション: `startRow` パラメータを使用して複数回取得可能
  - 1日のリクエスト数制限: あり（詳細は要確認）
- **実装の考慮事項**:
  - キーワード数が多い場合、ページネーション処理が必要
  - 非同期処理でユーザーに進捗を表示する
  - エラーハンドリング（API制限に達した場合など）

---

## アプローチ1: キーワードギャップ分析ベース（推奨）

### コンセプト

Search Consoleのデータから、**検索されているが自社サイトでカバーできていないキーワード**を特定し、それらをカバーする記事を提案する。

### 実装方法

#### ステップ1: キーワードギャップの特定

1. **GSC APIから全キーワードデータを取得**
   - `getKeywordData()` を拡張して、`pageUrl` を省略可能にする
   - `pageUrl` を指定しない場合、ドメイン全体のキーワードを取得
   - 過去90日間のデータを取得
   - インプレッション数が多いが、順位が低い（20位以下）キーワードを抽出
   
   **注意**: 現在の実装では `getKeywordData()` は `pageUrl` が必須のため、新規メソッド `getAllKeywords()` を追加する必要がある

2. **既存記事のキーワードカバレッジを分析**
   - 既存記事のタイトル・見出しから主要キーワードを抽出
   - 記事ごとのキーワードカバレッジをマッピング

3. **ギャップキーワードの特定**
   - 検索ボリューム（インプレッション）が高い
   - 自社サイトでカバーされていない
   - 順位が低い（20位以下）または未表示

#### ステップ2: 記事提案の生成

**方法A: キーワードクラスタリング（低コスト）**
- 類似キーワードをグループ化
- 各グループから1つの記事テーマを提案
- LLMを使わず、キーワードからタイトル案を自動生成

**方法B: LLMベースの提案（中コスト）**
- ギャップキーワードをLLMに渡す
- 既存記事のタイトル・構造を参考に提案
- 競合サイトの上位記事タイトルも参考に

### コスト分析

| 項目 | コスト | 備考 |
|------|--------|------|
| GSC API呼び出し | 無料 | 既存のAPIクライアントを使用 |
| キーワード分析 | 無料 | ローカル処理 |
| キーワードクラスタリング | 無料 | ローカル処理 |
| LLM API（方法B） | $0.01-0.05/提案 | Groq API使用時 |

### 精度評価

- **方法A**: ⭐⭐⭐ (3/5)
  - キーワードからタイトル案は生成可能
  - 内容の詳細提案は難しい
  
- **方法B**: ⭐⭐⭐⭐ (4/5)
  - 既存記事の構造を参考にできる
  - より具体的な提案が可能

### 実装のしやすさ

- **方法A**: ⭐⭐⭐⭐⭐ (5/5)
  - 既存のGSC APIクライアントを活用
  - 追加の外部API不要
  
- **方法B**: ⭐⭐⭐⭐ (4/5)
  - 既存のLLM統合を活用
  - プロンプト設計が必要

### 推奨実装

**フェーズ1（MVP）**: 方法Aで実装
- キーワードギャップ分析
- キーワードクラスタリング
- シンプルなタイトル提案

**フェーズ2（改善）**: 方法Bを追加
- LLMを使った詳細提案
- 既存記事の構造分析

---

## アプローチ2: 競合サイト分析ベース

### コンセプト

競合サイトで上位表示されている記事を分析し、自社サイトにないトピックを特定して提案する。

### 実装方法

1. **競合サイトの特定**
   - GSCデータから、同じキーワードで上位表示されているサイトを特定
   - または、ユーザーが競合サイトを指定

2. **競合サイトの記事一覧を取得**
   - サイトマップから記事URLを取得
   - または、検索結果から上位記事を抽出

3. **自社サイトとの比較**
   - 競合サイトの記事タイトルを分析
   - 自社サイトにないトピックを特定

4. **記事提案の生成**
   - 競合記事のタイトル・構造を参考に提案
   - LLMで自社サイトのトーンに合わせて調整

### コスト分析

| 項目 | コスト | 備考 |
|------|--------|------|
| サイトマップ取得 | 無料 | 標準的なHTTPリクエスト |
| 記事スクレイピング | 無料 | 既存のArticleScraperを使用 |
| 競合記事分析 | $0.01-0.05/記事 | LLM API使用時 |

### 精度評価

- ⭐⭐⭐⭐ (4/5)
  - 実際に上位表示されている記事を参考にできる
  - 競合分析の精度に依存

### 実装のしやすさ

- ⭐⭐⭐ (3/5)
  - サイトマップの取得が必要
  - 競合サイトの構造に依存
  - スクレイピングのコストが高い

### 課題

- 競合サイトの特定が難しい場合がある
- スクレイピングのコストが高い（時間・リソース）
- サイトマップが存在しない場合の対応が必要

---

## アプローチ3: トピックモデリングベース

### コンセプト

既存記事の内容を分析し、関連トピックでカバーされていない領域を特定する。

### 実装方法

1. **既存記事のトピック抽出**
   - 記事のタイトル・見出し・本文から主要トピックを抽出
   - TF-IDFやトピックモデリングを使用

2. **トピックマップの作成**
   - 既存記事をトピック空間にマッピング
   - カバーされていないトピック領域を特定

3. **記事提案の生成**
   - 未カバー領域のトピックから記事を提案
   - 関連キーワードを組み合わせてタイトル案を生成

### コスト分析

| 項目 | コスト | 備考 |
|------|--------|------|
| 記事スクレイピング | 無料 | 既存記事のみ |
| トピック分析 | 無料 | ローカル処理 |
| LLM API | $0.01-0.05/提案 | オプション |

### 精度評価

- ⭐⭐⭐ (3/5)
  - トピック抽出の精度に依存
  - 検索ボリュームとの関連付けが難しい

### 実装のしやすさ

- ⭐⭐⭐ (3/5)
  - トピックモデリングの実装が必要
  - 既存記事のスクレイピングが必要

### 課題

- 既存記事が少ない場合、精度が低い
- 検索ボリュームとの関連付けが必要

---

## アプローチ4: ハイブリッドアプローチ（最推奨）

### コンセプト

複数のアプローチを組み合わせて、より精度の高い提案を実現する。

### 実装方法

1. **キーワードギャップ分析（メイン）**
   - GSCデータからギャップキーワードを特定
   - インプレッション数で優先順位付け

2. **既存記事の構造分析（補助）**
   - 既存記事のタイトル・見出し構造を分析
   - 成功パターンを学習

3. **競合サイトの参考（オプション）**
   - ギャップキーワードで上位表示されている記事を参考
   - タイトル・構造を分析

4. **LLMによる提案生成**
   - 上記の情報を統合してLLMに渡す
   - 自社サイトのトーンに合わせた提案を生成

### コスト分析

| 項目 | コスト | 備考 |
|------|--------|------|
| GSC API | 無料 | 既存のAPIクライアント |
| キーワード分析 | 無料 | ローカル処理 |
| 既存記事分析 | 無料 | ローカル処理 |
| 競合記事分析 | $0.01-0.05/提案 | オプション |
| LLM API | $0.01-0.05/提案 | Groq API使用時 |

**合計**: $0.02-0.10/提案（競合分析含む）

### 精度評価

- ⭐⭐⭐⭐⭐ (5/5)
  - 複数のデータソースを統合
  - 検索ボリュームと既存記事の両方を考慮

### 実装のしやすさ

- ⭐⭐⭐⭐ (4/5)
  - 既存の機能を組み合わせる
  - 段階的な実装が可能

### 実装フェーズ

**フェーズ1（MVP）**
- キーワードギャップ分析
- シンプルなタイトル提案（LLMなし）

**フェーズ2（改善）**
- 既存記事の構造分析を追加
- LLMによる詳細提案

**フェーズ3（最適化）**
- 競合サイト分析を追加
- 提案の精度向上

---

## 実装詳細: ハイブリッドアプローチ（フェーズ1）

### データフロー

```
1. GSC API → 全キーワードデータ取得
2. 既存記事のキーワードカバレッジ分析
3. ギャップキーワードの特定
4. キーワードクラスタリング
5. タイトル案の生成（LLMなし）
6. DBに保存
7. ユーザーに提示（DBから取得）
```

### 使用パターン

**想定される使用フロー**:
1. ユーザーが「新規記事提案」を依頼
2. システムが10個程度の記事タイトル案を生成
3. 提案をDBに保存（`article_suggestions` テーブル）
4. ユーザーは提案一覧を確認
5. ユーザーが記事を書いたら、該当提案を「完了」としてマーク
6. 全ての提案が完了したら、再度提案依頼が可能

**特徴**:
- 頻繁には使用しない（数週間〜数ヶ月に1回程度）
- 一度に複数の提案（10個程度）を生成
- 提案はDBに保存して管理
- 提案の進捗を追跡可能

### 必要なAPI/機能

1. **GSC API拡張**
   - `getAllKeywords()`: ページ指定なしで全キーワード取得（新規メソッド）
   - 既存の `getKeywordData()` は `pageUrl` が必須のため、`pageUrl` を省略可能な新規メソッドが必要
   - 実装方法: `getKeywordData()` の `pageUrl` パラメータをオプショナルにして、未指定時はページフィルタを追加しない

2. **キーワード分析機能**
   - `analyzeKeywordGaps()`: ギャップキーワードの特定
   - `clusterKeywords()`: キーワードクラスタリング

3. **タイトル生成機能**
   - `generateTitleSuggestions()`: キーワードからタイトル案を生成（LLMなし）
   - 既存記事のタイトルパターンを学習

4. **DBスキーマ**
   - `article_suggestions` テーブル
     - `id`: UUID
     - `user_id`: ユーザーID
     - `site_id`: サイトID
     - `title`: 提案タイトル
     - `keywords`: キーワード配列（JSONB）
     - `outline`: 見出し構造（JSONB）
     - `reason`: 提案理由
     - `estimated_impressions`: 推定インプレッション数
     - `priority`: 優先度
     - `status`: ステータス（pending, in_progress, completed, skipped）
     - `created_at`: 作成日時
     - `completed_at`: 完了日時

### 実装例

```typescript
// lib/article-suggestion.ts

interface KeywordGap {
  keyword: string;
  impressions: number;
  position: number;
  clicks: number;
  ctr: number;
}

interface ArticleSuggestion {
  title: string;
  keywords: string[];
  outline?: string[];
  reason: string;
  estimatedImpressions: number;
  priority: number;
}

export class ArticleSuggestionGenerator {
  /**
   * URLからドメインを抽出してGSC形式に変換
   */
  private normalizeSiteUrl(input: string): string {
    // 既存のextractDomain関数を活用
    const { extractDomain } = require('./competitor-filter');
    
    try {
      // URLかドメインかを判定
      const url = input.startsWith('http') ? input : `https://${input}`;
      const urlObj = new URL(url);
      const domain = urlObj.hostname.replace(/^www\./, '');
      
      // GSC APIの形式に変換
      // sc-domain:形式を優先（ドメインプロパティ）
      return `sc-domain:${domain}`;
    } catch {
      // URLパースに失敗した場合、そのまま返す（既にGSC形式の可能性）
      return input;
    }
  }

  async generateSuggestions(
    siteUrlOrArticleUrl: string, // ドメインまたは記事URLのいずれか
    existingArticles: Article[],
    useLLM: boolean = false
  ): Promise<ArticleSuggestion[]> {
    // 1. 入力されたURLからドメインを抽出してGSC形式に変換
    const siteUrl = this.normalizeSiteUrl(siteUrlOrArticleUrl);
    
    // 2. GSCから全キーワード取得（ドメイン全体）
    // pageUrlを指定しないことで、ドメイン全体のキーワードを取得
    const allKeywords = await this.getAllKeywords(siteUrl);
    
    // 2. 既存記事のキーワードカバレッジ分析
    const coveredKeywords = this.analyzeCoverage(existingArticles);
    
    // 3. ギャップキーワードの特定
    const gaps = this.identifyGaps(allKeywords, coveredKeywords);
    
    // 4. キーワードクラスタリング
    const clusters = this.clusterKeywords(gaps);
    
    // 5. タイトル案の生成
    let suggestions: ArticleSuggestion[];
    if (useLLM) {
      // LLMを使用（フェーズ2）
      suggestions = await this.generateTitlesWithLLM(
        clusters, 
        existingArticles
      );
    } else {
      // LLMなし（フェーズ1）
      suggestions = this.generateTitles(clusters, existingArticles);
    }
    
    // 6. DBに保存
    await this.saveSuggestions(suggestions, userId, siteId);
    
    return suggestions;
  }

  private async generateTitlesWithLLM(
    clusters: KeywordCluster[],
    existingArticles: Article[]
  ): Promise<ArticleSuggestion[]> {
    // Gemini Flash 2.0を使用
    const provider = new GeminiLLMProvider();
    
    const prompt = this.buildLLMPrompt(clusters, existingArticles);
    const response = await provider.generateContent(prompt);
    
    // JSON形式でパース
    const parsed = JSON.parse(response);
    return parsed.suggestions; // 10個の提案を返す
  }

  private async getAllKeywords(siteUrl: string): Promise<KeywordGap[]> {
    // GSC APIからドメイン全体のキーワードを取得
    // pageUrlを指定しないことで、全ページのキーワードを取得
    const client = await getGSCClient();
    
    const endDate = new Date(Date.now() - 2 * 24 * 60 * 60 * 1000)
      .toISOString()
      .split("T")[0];
    const startDate = new Date(Date.now() - 90 * 24 * 60 * 60 * 1000)
      .toISOString()
      .split("T")[0];
    
    // ページネーション処理（最大25,000行まで1回で取得可能）
    const allKeywords: KeywordGap[] = [];
    const rowLimit = 25000; // GSC APIの最大値
    let startRow = 0;
    let hasMore = true;
    
    while (hasMore) {
      // pageUrlを指定せずに呼び出す（新規メソッドが必要）
      const keywordData = await client.getAllKeywords(
        siteUrl, 
        startDate, 
        endDate,
        rowLimit,
        startRow
      );
      
      const keywords = keywordData.rows.map((row) => ({
        keyword: row.keys[0],
        impressions: row.impressions,
        position: row.position,
        clicks: row.clicks,
        ctr: row.ctr,
      }));
      
      allKeywords.push(...keywords);
      
      // 次のページがあるかチェック
      if (keywordData.rows.length < rowLimit) {
        hasMore = false;
      } else {
        startRow += rowLimit;
      }
    }
    
    return allKeywords;
  }
}
```

### DBスキーマ例

```sql
-- 記事提案テーブル
CREATE TABLE article_suggestions (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  user_id UUID NOT NULL REFERENCES users(id) ON DELETE CASCADE,
  site_id UUID NOT NULL REFERENCES sites(id) ON DELETE CASCADE,
  title TEXT NOT NULL,
  keywords JSONB NOT NULL,
  outline JSONB,
  reason TEXT,
  estimated_impressions INTEGER,
  priority INTEGER DEFAULT 0,
  status TEXT NOT NULL DEFAULT 'pending' CHECK (status IN ('pending', 'in_progress', 'completed', 'skipped')),
  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
  updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
  completed_at TIMESTAMP WITH TIME ZONE,
  CONSTRAINT fk_user FOREIGN KEY (user_id) REFERENCES users(id),
  CONSTRAINT fk_site FOREIGN KEY (site_id) REFERENCES sites(id)
);

CREATE INDEX idx_article_suggestions_user_site ON article_suggestions(user_id, site_id);
CREATE INDEX idx_article_suggestions_status ON article_suggestions(status);
```

### コスト見積もり

- **開発時間**: 2-3日（フェーズ1）、+1-2日（フェーズ2）
- **APIコスト（フェーズ1）**: 無料（GSC APIのみ）
- **APIコスト（フェーズ2）**: 約$0.01-0.02/提案セット（10提案生成時）
- **運用コスト**: ほぼ無料（DB保存のみ）

### 処理時間の見積もり

**ドメイン全体のキーワード取得**:
- **小規模サイト**（キーワード数 < 1,000）: 1-3秒（1回のAPI呼び出し）
- **中規模サイト**（キーワード数 1,000-25,000）: 3-10秒（1回のAPI呼び出し、最大25,000行まで）
- **大規模サイト**（キーワード数 > 25,000）: 10-30秒（ページネーション、複数回のAPI呼び出し）

**全体の処理時間**（提案生成まで）:
- **フェーズ1（LLMなし）**: キーワード取得 + 分析 + タイトル生成 = 5-35秒
- **フェーズ2（LLMあり）**: 上記 + LLM処理（Gemini Flash 2.0） = 10-40秒

**最適化のポイント**:
- キーワード取得は非同期処理で進捗を表示
- 大量データの場合は、インプレッション数の閾値を設定してフィルタリング（例: 10以上）
- キャッシュを活用（同じサイトのデータは一定期間キャッシュ）

**注意事項**:
- GSC APIは1回のリクエストで最大25,000行まで取得可能
- それ以上の場合、`startRow` パラメータでページネーションが必要
- API呼び出しは「一瞬」ではなく、データ量に応じて数秒〜数十秒かかる可能性がある
- ただし、コストは無料で、手間も最小限（API呼び出しのみ）

### 使用頻度の想定

- **提案生成**: 数週間〜数ヶ月に1回程度
- **提案数**: 1回あたり10個程度
- **完了までの期間**: 数週間〜数ヶ月（ユーザーが記事を書くペースによる）
- **再提案のタイミング**: 全ての提案が完了したら再度依頼可能

---

## 実装詳細: ハイブリッドアプローチ（フェーズ2）

### LLM統合

既存のLLM統合（Gemini Flash 2.0）を活用して、より詳細な提案を生成。

**使用モデル**: `gemini-2.0-flash-exp` または `gemini-2.0-flash-lite`
- 既存の `GeminiLLMProvider` を活用
- 環境変数 `GEMINI_MODEL` でモデル選択可能（デフォルト: `gemini-2.0-flash-lite`）

### プロンプト設計

```
あなたはSEOコンテンツ戦略の専門家です。

以下の情報を基に、新規記事の提案をしてください：

## ギャップキーワード
{ギャップキーワードのリスト}

## 既存記事のタイトル例
{既存記事のタイトルリスト}

## 既存記事の見出し構造例
{既存記事の見出し構造}

## 要件
1. ギャップキーワードをカバーする記事タイトルを**10個**提案してください
2. 各タイトルは異なる観点・アプローチから提案してください
3. 各タイトルについて、記事の構成案（見出し構造）を提示してください
4. 既存記事のトーンと一貫性を保ってください
5. SEO効果が期待できる内容にしてください

## 出力形式
{
  "suggestions": [
    {
      "title": "記事タイトル",
      "keywords": ["キーワード1", "キーワード2"],
      "outline": [
        "H2: 見出し1",
        "H2: 見出し2",
        "H3: 見出し2-1"
      ],
      "reason": "この記事を書くべき理由",
      "estimatedImpressions": 1000,
      "priority": 1
    }
  ]
}
```

### コスト見積もり

- **開発時間**: 1-2日
- **APIコスト**: 約$0.01-0.02/提案セット（Gemini Flash 2.0、10提案生成時）
  - Gemini 2.0 Flash Lite: 約$0.50/1Mトークン
  - 1提案セットあたり約20,000-40,000トークン想定
- **運用コスト**: ユーザーが提案を生成するたびに発生（頻繁には使用しない想定）

---

## 精度向上のための工夫

### 1. キーワードの優先順位付け

- **インプレッション数**: 検索ボリュームが高いキーワードを優先
- **CTR**: クリック率が高いキーワードを優先
- **順位**: 順位が低い（20位以下）キーワードを優先
- **トレンド**: 最近インプレッションが増加しているキーワードを優先

### 2. 既存記事の成功パターン分析

- 既存記事のタイトルパターンを分析
- 順位が良い記事の構造を学習
- 成功パターンを新規提案に反映

### 3. 競合サイトの参考

- ギャップキーワードで上位表示されている記事を分析
- タイトル・構造を参考に提案

### 4. ユーザーフィードバック

- 提案された記事を実際に作成したか
- 作成した記事の順位変化を追跡
- フィードバックを学習データとして活用

---

## コスパ評価

### コスト

| 項目 | コスト |
|------|--------|
| 開発時間 | 3-5日 |
| APIコスト（フェーズ1） | 無料 |
| APIコスト（フェーズ2） | $0.01-0.05/提案 |
| 運用コスト | ほぼ無料 |

### 効果

- **ユーザー価値**: ⭐⭐⭐⭐ (4/5)
  - 新規記事のネタ探しの時間短縮
  - SEO効果が期待できる記事の提案
  
- **ビジネス価値**: ⭐⭐⭐⭐ (4/5)
  - ユーザーのエンゲージメント向上
  - 有料プランへのアップセル材料

### 結論

**コスパ: ⭐⭐⭐⭐⭐ (5/5)**

- フェーズ1はほぼ無料で実装可能
- フェーズ2も低コストで実装可能
- ユーザー価値が高い
- 段階的な実装が可能

---

## 実装優先度

1. **高**: フェーズ1（キーワードギャップ分析 + シンプルなタイトル提案）
2. **中**: フェーズ2（LLM統合）
3. **低**: フェーズ3（競合サイト分析）

---

## 実装前に確認・詰めるべき点

### 1. 既存記事の取得方法

**課題**: 既存記事のキーワードカバレッジを分析する必要があるが、どのように取得するか？

**選択肢**:
- **A. DBから取得**（推奨）
  - `getArticlesBySiteId()` または `getArticlesByUserId()` を使用
  - メリット: 既存の実装を活用、コスト無料
  - デメリット: DBに登録されていない記事は対象外
  - **推奨**: まずはDBから取得。DBに登録されていない記事は後で対応

- **B. GSC APIからページURL一覧を取得**
  - `getPageUrls()` を使用して全ページURLを取得
  - メリット: ドメイン内の全記事を網羅
  - デメリット: タイトル・見出しの取得にスクレイピングが必要（コストがかかる）
  - **将来の拡張**: フェーズ3で検討

**結論**: フェーズ1では**A. DBから取得**を採用。DBに登録されていない記事は対象外とする。

### 2. 既存記事のキーワードカバレッジ分析

**課題**: 既存記事のタイトル・見出しからキーワードを抽出する方法

**選択肢**:
- **A. タイトルのみから抽出**（フェーズ1、推奨）
  - DBに保存されている `title` フィールドからキーワードを抽出
  - メリット: コスト無料、高速
  - デメリット: 精度が低い可能性
  - **実装方法**: タイトルを単語分割してキーワードを抽出

- **B. タイトル + 見出し構造から抽出**（フェーズ2以降）
  - 記事をスクレイピングして見出し構造を取得
  - メリット: 精度が高い
  - デメリット: スクレイピングのコストがかかる（時間・リソース）
  - **実装方法**: 既存の `ArticleScraper` を活用

**結論**: フェーズ1では**A. タイトルのみから抽出**を採用。フェーズ2でLLM統合時に改善。

### 3. キーワードクラスタリング

**課題**: 類似キーワードをグループ化して記事テーマを提案する方法

**既存実装の活用**:
- `KeywordPrioritizer` クラスを活用
  - `normalizeKeyword()`: キーワードの正規化
  - `groupAndSelectKeywords()`: キーワードのグループ化
  - `prioritizeKeywords()`: キーワードの優先順位付け

**実装方法**:
- ギャップキーワードを `groupAndSelectKeywords()` でグループ化
- 各グループから1つの記事テーマを提案
- タイトル案は、グループの代表キーワードから生成

### 4. エラーハンドリング

**GSC APIの制限（2025年現在）**:
- **サイト単位**: 1,200 QPM（1分あたり1,200リクエスト）
- **ユーザー単位**: 1,200 QPM（1分あたり1,200リクエスト）
- **プロジェクト単位**: 100,000,000 QPD（1日あたり1億リクエスト）
- **1リクエストあたりの最大行数**: 50,000行
- **URL検査API**: 2,000回/日（プロパティあたり）

**確認すべき点**:
- GSC APIの1日のリクエスト数制限（✅ 確認済み）
- データが少ない場合の処理（キーワード数が少ない、既存記事が少ない）
- API呼び出し失敗時のリトライ処理（429エラー時の指数バックオフ）

**対策**:
- エラーメッセージを明確に表示
- データが少ない場合は警告を表示しつつ、可能な範囲で提案
- API制限に達した場合（429エラー）は、指数バックオフでリトライ
- リトライ後も失敗する場合は、ユーザーに通知
- 1リクエストで50,000行を超える場合は、ページネーション処理を実装

### 5. UI/UX

**決定**: 分析開始画面（`app/[locale]/page.tsx`）にタブを追加

**実装方針**:
- **タブ構成**:
  - タブ1: 「記事分析」（既存の分析機能）
  - タブ2: 「記事提案」（新規記事提案機能）
- **デフォルト**: 「記事分析」タブを表示
- **タブ切り替え**: タブをクリックすると、それぞれのUIを表示

**記事提案タブのUI**:
- サイト選択（GSCプロパティ選択、既存の実装を活用）
- 「記事提案を生成」ボタン
- 進捗表示（キーワード取得中、分析中、提案生成中）
- 提案一覧（カード形式）
  - 各提案に「記事を作成」「スキップ」ボタンを配置
  - ステータス表示（pending, in_progress, completed, skipped）

**提案一覧の表示場所**:
- タブ内に表示（同じページ内）
- または、ダッシュボードに「記事提案」ページを追加して、詳細一覧を表示（要検討）

**推奨実装**:
- タブ内で提案生成と簡易一覧を表示
- 詳細な提案一覧は、ダッシュボードの「記事提案」ページで管理

### 6. データの整合性

**確認すべき点**:
- サイトIDとドメインの整合性
- ユーザーが入力したドメインが、登録済みのサイトと一致するか
- 既存記事のサイトIDが正しいか

**対策**:
- 入力されたドメインが登録済みサイトと一致するかチェック
- 一致しない場合は、新規サイトとして登録を促す
- 既存記事のサイトIDを確認し、必要に応じて修正

---

## リスクと対策

### リスク1: 提案の精度が低い

**対策**:
- ユーザーフィードバックを収集
- 提案の精度を段階的に改善
- 複数の提案を提示して選択肢を提供

### リスク2: GSC APIの制限

**対策**:
- API呼び出し回数を最小化
- キャッシュを活用
- バッチ処理で効率化

### リスク3: 既存記事が少ない場合の精度低下

**対策**:
- 既存記事が少ない場合は、競合サイト分析を優先
- キーワードギャップ分析のみでも提案可能

---

## まとめ

**推奨アプローチ**: ハイブリッドアプローチ（フェーズ1から段階的に実装）

**理由**:
1. コストが低い（フェーズ1はほぼ無料、フェーズ2も低コスト）
2. 既存の機能を活用できる（GSC API、Gemini Flash 2.0）
3. 段階的な実装が可能
4. 精度向上の余地がある
5. ユーザー価値が高い
6. 使用頻度が低いため、コスト負担が少ない

**実装のポイント**:
1. **複数提案の生成**: 1回の依頼で10個程度の記事タイトル案を生成
2. **DB保存**: 提案はDBに保存して管理し、進捗を追跡可能にする
3. **使用パターン**: 全ての提案が完了したら再度提案依頼が可能
4. **LLM使用**: Gemini Flash 2.0を使用（既存実装を活用）

**実装前の確認事項**:
1. ✅ GSC APIでドメイン全体のキーワード取得が可能（`pageUrl` を省略）
2. ✅ 既存の `KeywordPrioritizer` を活用可能
3. ✅ 既存記事はDBから取得（`getArticlesBySiteId()`）
4. ⚠️ 既存記事のキーワード抽出はタイトルのみ（フェーズ1）
5. ✅ GSC APIの1日のリクエスト数制限を確認（確認済み）
   - サイト単位: 1,200 QPM
   - プロジェクト単位: 100,000,000 QPD
   - 1リクエストあたり最大50,000行
6. ✅ UI/UXの設計（分析開始画面にタブを追加）

**次のステップ**:
1. **実装前の確認**:
   - GSC APIの1日のリクエスト数制限を確認
   - UI/UXの設計（提案一覧ページのレイアウト）
   - エラーハンドリングの方針を決定

2. **DBスキーマの設計・実装**:
   - `article_suggestions` テーブルのマイグレーション作成
   - インデックスの設計

3. **フェーズ1の実装**:
   - GSC API拡張（`getAllKeywords()` メソッド追加）
   - キーワードギャップ分析機能
   - キーワードクラスタリング（既存の `KeywordPrioritizer` を活用）
   - シンプルなタイトル提案（LLMなし）
   - DB保存機能
   - APIエンドポイント作成

4. **UI実装**:
   - 提案一覧ページの作成
   - 進捗表示
   - 提案のステータス管理

5. **ユーザーテスト**:
   - 実際のデータでテスト
   - フィードバック収集

6. **フェーズ2の実装**（改善）:
   - Gemini Flash 2.0統合で詳細提案
   - 既存記事の見出し構造分析（オプション）

